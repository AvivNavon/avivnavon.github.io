<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Auxiliary Learning by Implicit Differentiation">
    <meta name="author" content="
    Aviv Navon, 
    Idan Achituve, 
    Haggai Maron, 
    Gal Chechik,   
    Ethan Fetaya"
    >

    <title>Auxiliary Learning by Implicit Differentiation</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
</head>

<!-- MathJax -->
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>


<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h1>*** WIP ***</h1>
    <h1>Auxiliary Learning by Implicit Differentiation</h1>
    <hr>
    <p class="authors">
        <tr>
            <span style="font-size:24px"><a href="https://avivnavon.github.io/">Aviv Navon*</a><sup>1</sup></span> &nbsp;
            <span style="font-size:24px"><a href="https://chechiklab.biu.ac.il/~achitui/">Idan Achituve*</a><sup>1</sup></span>&nbsp;
            <span style="font-size:24px"><a href="https://haggaim.github.io/">Haggai Maron</a><sup>2</sup></span> &nbsp;
        </tr><br>
        <tr>
            <span style="font-size:24px"><a href="https://chechiklab.biu.ac.il/">Gal Chechik<sup>†</sup></a><sup>1,2</sup></span> &nbsp;
            <span style="font-size:24px"><a href="http://www.eng.biu.ac.il/fetayae/">Ethan Fetaya<sup>†</sup></a><sup>1</sup></span> &nbsp;
        </tr>
    </p>

    <span style="font-size:18px"><sup>*,†</sup>Equally contributed</span>
    <br>
    <br>
    <table align=center>
        <tr>
            <td align=center>
                <center>
                    <span style="font-size:20px">
                        <sup>1</sup>Bar-Ilan University
                        <br>
                        <sup>2</sup>NVIDIA Research
                    </span>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <br>

    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2007.02693">Paper</a>
        <a class="btn btn-primary" href="https://github.com/AvivNavon/AuxiLearn">Code</a>
    </div>

    

</div>

<!-- /Users/avivnavon/Desktop/avivnavon.github.io -->

<div class="container">
    <div class="section">
         <center><img src="/AuxiLearn/resources/framework.png" align="middle" style='max-width: 100%'> </center>
        <hr>
        <p>
            Training with multiple auxiliary tasks is a common practice used in deep learning for improving the performance on the main task of interest. Two main challenges arise in this multi-task learning setting: (i) Designing useful auxiliary tasks; and (ii) Combining auxiliary tasks into a single coherent loss. We propose a novel framework, <i>AuxiLearn</i>, that targets both challenges, based on implicit differentiation. First, when useful auxiliaries are known, we propose learning a network that combines all losses into a single coherent objective function. This network can learn <i>non-linear</i> interactions between auxiliary tasks. Second, when no useful auxiliary task is known, we describe how to learn a network that generates a meaningful, novel auxiliary task. We evaluate AuxiLearn in a series of tasks and domains, including image segmentation and learning with attributes. We find that AuxiLearn consistently improves accuracy compared with competing methods.
        </p>
    </div>

    <div class="section">
        <h2>Unified framework for auxiliary learning</h2>
        <hr>
        <p>
            Our goal is to find optimal auxiliary parameters $\phi$ ...
        </p>
    </div>

    <div class="section">
        <center><h2>Nonliear loss combinations</h2></center>
        <hr>
        <h3>An illustrative example</h3>
        <hr>
        <p>
            <figure>
            <center>
                <figcaption>Loss landscape generated by the auxiliary network: (a) main task only (b); beginning of training; and (c) at convergence.</figcaption>
                <img src="/AuxiLearn/resources/illus.png" align="middle" style='max-width: 100%'> 
            </center>
            </figure>

            We first present an illustrative example of how AuxiLearn changes the loss landscape and helps generalization in the presence of label noise and harmful tasks. Here $W^\star$ is the optimal solution, and $W_{train}$ is the STL solution on the train set (main task only). We train a linear auxiliary network to output task weights over the losses. The auxiliary network learns to ignore the harmful auxiliary and use the helpful one to find a better solution by changing the loss landscape.

          <!--   <figure>
            <center>
                <figcaption></figcaption>
                <img src="/Users/avivnavon/Desktop/avivnavon.github.io/AuxiLearn/resources/loss_landscape.gif" align="middle" style='max-width: 100%'> 
            </center>
            </figure> -->
            
        </p>
    <!-- <div class="section"> -->
        <br>
        <h3>Learning with many auxiliaries</h3>
        <hr>
        <p>

         We evaluated AuxiLearn on the task of fine-grained classification of bird species (CUB-200 dataset) using <b>200 auxiliary tasks</b>. Each image is associated with a specie (one of 200) and a set of 312 binary visual attributes, which we use as auxiliaries. Here we focus on a semi-supervised setting, in which auxiliary labels are available for all images but only 5 and 10 labels per class of the main task (noted as 5-shot and 10-shot, respectively).

         <figure>
            <center>
                <img src="/AuxiLearn/resources/cub_table.png" align="middle" style='max-width: 60%'> 
                <figcaption></figcaption>
            </center>
            </figure>

        </p>
        
    <!-- </div> -->

    <!-- <div class="section"> -->
        <br>
        <h3>Convolutional loss network for pixel-wise tasks</h3>
        <hr>
        <center>
            <figcaption><i>Loss images</i>: (a) original image; (b) semantic segmentation ground truth; (c) auxiliaries loss; (d) segmentation (main task) loss; (e) adaptive pixel-wise weight $\sum_j\partial \mathcal{L}_T/\partial \ell_j$.</figcaption>
            <img src="/AuxiLearn/resources/nyu.png" align="middle" style='max-width: 100%'> 
        </center>

        <p>
            In certain problems, there exist a spatial relation among losses. For example, consider the tasks of semantic segmentation, depth estimation and surface-normal estimation for images. The common approach is to average the losses over all locations. We can, however, leverage this spatial relation for creating a <i>loss-image</i>, in which each task forms a channel of pixel-losses induced by the task. We can now stack those channels and parametrizegas a CNN that acts on this loss-image. Here we evaluate AuxiLearn on the main task of semantic segmentation on the NYUv2 dataset.
        </p>

        <figure>
            <center>
                <img src="/AuxiLearn/resources/nyu_table.png" align="middle" style='max-width: 55%'> 
                <figcaption></figcaption>
            </center>
        </figure>

    <!-- </div> -->

    <div class="section">
        <h2>Learning a novel classification auxiliary task</h2>
        <hr>
        <p>
            <figure>
            <center>
                <figcaption>t-SNE applied to the auxiliaries learned for the <i>Frog</i> and <i>Deer</i> classes, in CIFAR10.</figcaption>
                <img src="/AuxiLearn/resources/cifar10_labels_dense.png" align="middle" style='max-width: 100%'> 
            </center>
        </figure>
            
            The previous subsection focused on a case where auxiliary tasks are given. In many cases, however, no useful auxiliary tasks are known in advance, and we are only presented with the main task. How can we utilize the benefits of auxiliary learning in such cases? We propose using our framework to generate an auxiliary task with the auxiliary network and learning the main task and the auxiliary task with the primary network. The learned auxiliary task is tailor-made to help the learning of the main task.
        </p>

    </div>


    <div class="section">
        <h2>Paper</h2>
        <hr>
        <div>
            <div class="list-group">
                <a href="https://arxiv.org/abs/2007.02693"
                   class="list-group-item">
                    <img src="/AuxiLearn/resources/paper.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
    @article{navon2020auxiliary,
      title={Auxiliary Learning by Implicit Differentiation},
      author={Navon, Aviv and Achituve, Idan and Maron, Haggai and Chechik, Gal and Fetaya, Ethan},
      journal={arXiv preprint arXiv:2007.02693},
      year={2020}
    }
        </div>
    </div>

    <hr>

    <footer>
    </footer>
</div>


<div class="section">
        <h2>Acknowledgements</h2>
        <hr>
        <p>
             This study was funded by a grant to GC from the Israel Science Foundation (ISF 737/2018), and by an equipment grant to GC and Bar-Ilan University from the Israel Science Foundation (ISF 2332/18). IA  was funded by a grant from the Israeli innovation authority, through the AVATAR consortium.
        </p>

    </div>



<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

    <footer>The website template was borrowed from <a href="https://vsitzmann.github.io/">Vincent Sitzmann</a>.</footer>

</body>
</html>